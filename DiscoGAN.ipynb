{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chimeras - A mixture of red pandas and lions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By Eneritz Dom√≠nguez and Ander Salaberria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During this project, we have had in mind two different approaches to achieve our goal. We'll show our built DiscoGAN after explaining how we got the images, and then we'll show a fully connected neural network that learns to differenciate between red pandas, lions and the background of the image (image segmentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the needed images and models are saved in GitHub. It is preferable to download it, and use the notebook that is already in that repository. Check it out here: http://github.com/eneritznely/Chimeras.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed in part 1\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# imports needed in part 2\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# imports needed in part 3\n",
    "import numpy as np\n",
    "import scipy.misc as misc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Getting Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two blocks and a .txt file are enough to get the images we need. These .txt contains a list of URL where images of one class are contained (lion, for instance). Those lists were extracted from www.image-net.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image from URL\n",
    "def getImage(url, imgID, imgTarget):\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "    except:\n",
    "        return \"fail request - \" + str(imgID)\n",
    "    try:\n",
    "        img = Image.open(BytesIO(response.content))\n",
    "    except:\n",
    "        return \"no image - \" + str(imgID)\n",
    "    \n",
    "    file_name = str(imgTarget) + \"_\" + str(imgID) + \".jpeg\"\n",
    "    \n",
    "    try:\n",
    "        img.save(file_name);\n",
    "    except:\n",
    "        return \"fail save - \" + str(imgID)\n",
    "    \n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Lion\"\n",
    "images = open(\"Datasets/Lion_URL.txt\", \"r\")\n",
    "if not os.path.exists(target):\n",
    "    os.mkdir(target)\n",
    "os.chdir(target)\n",
    "i = 0\n",
    "for line in images:\n",
    "    i = i + 1\n",
    "    response = getImage(line, i, target)\n",
    "    if not response == \"\":\n",
    "        print(response)\n",
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model only deals with 64x64 images, so we need to resize them. It is necessary to change the directory of variable \"path\" in order to specify where is our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resizes all the images in an directory \"path\" into 64x64 size \n",
    "def resize(path):\n",
    "    dirs = os.listdir(path)\n",
    "    for item in dirs:\n",
    "        if os.path.isfile(path+item):\n",
    "            im = Image.open(path+item)\n",
    "            f, e = os.path.splitext(path+item)\n",
    "            imResize = im.resize((64,64), Image.ANTIALIAS)\n",
    "            imResize.save(f + '.jpeg', 'JPEG', quality=90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"Lion/\"\n",
    "resize(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. DISCOvery Cross-Domain Relations with GANs (DiscoGAN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This implementation is in Tensorflow, so, firstly, we are going to define our hyperparameters and reset the tensorflow graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Hyperparameters\n",
    "initializer = tf.truncated_normal_initializer(stddev=0.02) # define weights with mean 0 and standard deviaton of 0.02\n",
    "learning_rate = 0.0005\n",
    "batch_size = 250\n",
    "epoch = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing input images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, we are going to detect and read both red panda and lion images. We'll do all this with tensorflow's tensors. So, these cells are going to define the first part of the graph where all images are read and preprocessed in an appropiate way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images \n",
    "red_pandas_filename_queue = tf.train.string_input_producer(tf.train.match_filenames_once(\"Red_panda_64x64/*.jpeg\"), capacity=200)\n",
    "lions_filename_queue = tf.train.string_input_producer(tf.train.match_filenames_once(\"Lion_64x64/*.jpeg\"), capacity=200)\n",
    "\n",
    "image_reader = tf.WholeFileReader()\n",
    "_, red_pandas_file = image_reader.read(red_pandas_filename_queue)\n",
    "_, lions_file = image_reader.read(lions_filename_queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decode images so each image is composed by width x height x channel integers/pixel-values\n",
    "red_pandas_image = tf.image.decode_jpeg(red_pandas_file)\n",
    "lions_image = tf.image.decode_jpeg(lions_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will normalize the pixel values, so black stays as RGB = [0.0, 0.0, 0.0] and white becomes RGB = [1.0, 1.0, 1.0]\n",
    "red_pandas_image = tf.cast(tf.reshape(red_pandas_image,shape=[64,64,3]),dtype=tf.float32)/255.0\n",
    "lions_image = tf.cast(tf.reshape(lions_image,shape=[64,64,3]),dtype=tf.float32)/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the shuffle_batch function, we are going to create a batch which is different on each iteration, beacuse they are randomly shuffled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_preprocess_threads = 1 #number of threads used to shuffle the batches\n",
    "min_queue_examples = 250 # min number of elements admitted in queue\n",
    "\n",
    "batch_red_pandas = tf.train.shuffle_batch([red_pandas_image], # images which will be contained in the batch\n",
    "                                     batch_size=batch_size,   # number of images in a batch\n",
    "                                     num_threads=num_preprocess_threads, # number of threads that will execute this\n",
    "                                     capacity=min_queue_examples + 3 * batch_size, # max number of elements in queue\n",
    "                                     min_after_dequeue=min_queue_examples) # min elements in queue after a dequeue\n",
    "\n",
    "batch_lions = tf.train.shuffle_batch([lions_image],\n",
    "                                    batch_size=batch_size,\n",
    "                                    num_threads=num_preprocess_threads,\n",
    "                                    capacity=min_queue_examples + 3 * batch_size,\n",
    "                                    min_after_dequeue=min_queue_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining neural networks - Generators & Discriminators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have all the preprocessing defined, let's define our discriminators and generators of the DiscoGAN. \n",
    "\n",
    "On the one hand, we've got two discriminators, which are CNNs with 5 convolutional layers each, followed by 2 fully connected layers. We use the ReLU activation function (except in the last layer, where we use the sigmoid function), kernel size of 4 and stride of 2. The weights are initialized as defined in the hyperparameters and a normalization function is used instead of biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with this function we define a discriminator, using the following parameters:\n",
    "#   - tensor: the input tensor of the image\n",
    "#   - name: the name we'll use to specify the discriminator we are creating/reusing\n",
    "#   - reuse: if the discriminator isn't already created (FALSE) or if we'll be using a existing discriminator (TRUE)\n",
    "\n",
    "def discriminator(tensor,name,reuse=False):\n",
    "    \n",
    "    with tf.variable_scope(name):\n",
    "        conv1 = tf.contrib.layers.conv2d(inputs=tensor, num_outputs=32, kernel_size=4, stride=2, padding=\"SAME\", \\\n",
    "                                        reuse=reuse, activation_fn=tf.nn.relu,weights_initializer=initializer,scope=\"d_conv1\") # 32 x 32 x 32\n",
    "        conv2 = tf.contrib.layers.conv2d(inputs=conv1, num_outputs=64, kernel_size=4, stride=2, padding=\"SAME\", \\\n",
    "                                        reuse=reuse, activation_fn=tf.nn.relu,normalizer_fn=tf.contrib.layers.batch_norm,\\\n",
    "                                        weights_initializer=initializer,scope=\"d_conv2\")                                  # 16 x 16 x 64\n",
    "        conv3 = tf.contrib.layers.conv2d(inputs=conv2, num_outputs=128, kernel_size=4, stride=2, padding=\"SAME\", \\\n",
    "                                        reuse=reuse, activation_fn=tf.nn.relu,normalizer_fn=tf.contrib.layers.batch_norm,\\\n",
    "                                        weights_initializer=initializer,scope=\"d_conv3\")                                  # 8 x 8 x 128 \n",
    "        conv4 = tf.contrib.layers.conv2d(inputs=conv3, num_outputs=256, kernel_size=4, stride=2, padding=\"SAME\", \\\n",
    "                                        reuse=reuse, activation_fn=tf.nn.relu,normalizer_fn=tf.contrib.layers.batch_norm,\\\n",
    "                                        weights_initializer=initializer,scope=\"d_conv4\")                                  # 4 x 4 x 256\n",
    "        conv5 = tf.contrib.layers.conv2d(inputs=conv4, num_outputs=512, kernel_size=4, stride=2, padding=\"SAME\", \\\n",
    "                                        reuse=reuse, activation_fn=tf.nn.relu,normalizer_fn=tf.contrib.layers.batch_norm,\\\n",
    "                                        weights_initializer=initializer,scope=\"d_conv5\")                                  # 2 x 2 x 512\n",
    "        fc1 = tf.reshape(conv5, shape=[batch_size, 2*2*512])\n",
    "        fc1 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=512,reuse=reuse, activation_fn=tf.nn.relu, \\\n",
    "                                                normalizer_fn=tf.contrib.layers.batch_norm, \\\n",
    "                                                weights_initializer=initializer,scope=\"d_fc1\")\n",
    "        fc2 = tf.contrib.layers.fully_connected(inputs=fc1, num_outputs=1, reuse=reuse, activation_fn=tf.nn.sigmoid,\\\n",
    "                                                weights_initializer=initializer,scope=\"d_fc2\")\n",
    "\n",
    "        return fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, we've got our four generators. These have an encoder-decoder architecture meaning that both the input and output are images. We use 4 encoders (convolution + activation function) to hopefully get a higher level representation of the image. Another 4 decode layers do the opposite (deconvolution + activation function) and reverse the action of the encoder layers.\n",
    "\n",
    "In these case, all layers use our beloved ReLU activation function and kernel size of 4. The stride is 2 for encoder layers and 1 for decoder layers. After each deconvolution, a reshape is done to increase the dimensions (height and width) of the image, reducing the channel number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with this function we define a generator, using the following parameters:\n",
    "#   - image: the input tensor of the image\n",
    "#   - name: the name we'll use to specify the generator we are creating/reusing\n",
    "#   - reuse: if the generator isn't already created (FALSE) or if we'll be using a existing discriminator (TRUE)\n",
    "\n",
    "def generator(image,name,reuse=False):\n",
    "\n",
    "    with tf.variable_scope(name,reuse=reuse):\n",
    "        conv1 = tf.contrib.layers.conv2d(inputs=image, num_outputs=32, kernel_size=4, stride=2, padding=\"SAME\", \\\n",
    "                                        reuse=reuse, activation_fn=tf.nn.relu,weights_initializer=initializer,scope=\"d_conv1\") # 32 x 32 x 32\n",
    "        conv2 = tf.contrib.layers.conv2d(inputs=conv1, num_outputs=64, kernel_size=4, stride=2, padding=\"SAME\", \\\n",
    "                                        reuse=reuse, activation_fn=tf.nn.relu,normalizer_fn=tf.contrib.layers.batch_norm,\\\n",
    "                                        weights_initializer=initializer,scope=\"d_conv2\")                                  # 16 x 16 x 64\n",
    "        conv3 = tf.contrib.layers.conv2d(inputs=conv2, num_outputs=128, kernel_size=4, stride=2, padding=\"SAME\", \\\n",
    "                                        reuse=reuse, activation_fn=tf.nn.relu,normalizer_fn=tf.contrib.layers.batch_norm,\\\n",
    "                                        weights_initializer=initializer,scope=\"d_conv3\")                                  # 8 x 8 x 128 \n",
    "        conv4 = tf.contrib.layers.conv2d(inputs=conv3, num_outputs=256, kernel_size=4, stride=2, padding=\"SAME\", \\\n",
    "                                        reuse=reuse, activation_fn=tf.nn.relu,normalizer_fn=tf.contrib.layers.batch_norm,\\\n",
    "                                        weights_initializer=initializer,scope=\"d_conv4\")                                  # 4 x 4 x 256\n",
    "        \n",
    "        conv_trans1 = tf.contrib.layers.conv2d(conv4, num_outputs=4*128, kernel_size=4, stride=1, padding=\"SAME\",    \\\n",
    "                                        activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm, \\\n",
    "                                        weights_initializer=initializer,scope=\"g_conv1\")                            \n",
    "        conv_trans1 = tf.reshape(conv_trans1, shape=[batch_size,8,8,128])                                                 # 8 x 8 x 128\n",
    "\n",
    "        conv_trans2 = tf.contrib.layers.conv2d(conv_trans1, num_outputs=4*64, kernel_size=4, stride=1, padding=\"SAME\", \\\n",
    "                                        activation_fn=tf.nn.relu,normalizer_fn=tf.contrib.layers.batch_norm, \\\n",
    "                                        weights_initializer=initializer,scope=\"g_conv2\")\n",
    "        conv_trans2 = tf.reshape(conv_trans2, shape=[batch_size,16,16,64])                                                # 16 x 16 x 128\n",
    "\n",
    "        conv_trans3 = tf.contrib.layers.conv2d(conv_trans2, num_outputs=4*32, kernel_size=4, stride=1, padding=\"SAME\",    \\\n",
    "                                        activation_fn=tf.nn.relu, normalizer_fn=tf.contrib.layers.batch_norm, \\\n",
    "                                        weights_initializer=initializer,scope=\"g_conv3\")\n",
    "        conv_trans3 = tf.reshape(conv_trans3, shape=[batch_size,32,32,32])                                                # 32 x 32 x 32\n",
    "\n",
    "        conv_trans4 = tf.contrib.layers.conv2d(conv_trans3, num_outputs=4*16, kernel_size=4, stride=1, padding=\"SAME\", \\\n",
    "                                        activation_fn=tf.nn.relu,normalizer_fn=tf.contrib.layers.batch_norm, \\\n",
    "                                        weights_initializer=initializer,scope=\"g_conv4\")\n",
    "        conv_trans4 = tf.reshape(conv_trans4, shape=[batch_size,64,64,16])                                                # 64 x 64 x 16\n",
    "\n",
    "        recon = tf.contrib.layers.conv2d(conv_trans4, num_outputs=3, kernel_size=4, stride=1, padding=\"SAME\", \\\n",
    "                                        activation_fn=tf.nn.relu,scope=\"g_conv5\")                                         # 64 x 64 x 3\n",
    "\n",
    "\n",
    "        return recon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to build our DNN architecture (DiscoGAN), describing our generators, discriminators and connections between then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# DiscoGAN Architecture #\n",
    "#########################\n",
    "\n",
    "gen_l_fake = generator(batch_red_pandas, \"generator_rp_to_l\") # picks a real red panda image and creates a fake lion \n",
    "gen_rp_fake = generator(batch_lions, \"generator_l_to_rp\") # picks a real lion image and creates a fake red panda \n",
    "\n",
    "# these two generators share weights and characteristics with the first two generators\n",
    "recon_rp = generator(gen_l_fake, \"generator_l_to_rp\", reuse=True) # takes a fake chimera and tries to get the real panda\n",
    "recon_l = generator(gen_rp_fake, \"generator_rp_to_l\", reuse=True) # same but the chimera was based on a lion\n",
    "\n",
    "# Discriminators\n",
    "disc_rp_fake = discriminator(gen_rp_fake, \"discriminator_rp\") # defining discriminator with red panda fake inputs\n",
    "disc_l_fake = discriminator(gen_l_fake, \"discriminator_l\") # defining discriminator with lion fake inputs\n",
    "\n",
    "# these two discriminators are the same as the first two (shares weights and characteristics), but we add them\n",
    "# another possible input, real images\n",
    "disc_rp_real = discriminator(batch_red_pandas, \"discriminator_rp\", reuse=True) \n",
    "disc_l_real = discriminator(batch_lions, \"discriminator_l\", reuse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing loss and gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function is calculated differently for generators and discriminators. Discriminator's loss is only determined by computing sigmoid cross entropy with logits (the inverse function of the sigmoid). Generators loss is calculated using the sigmoid cross entropy, as well, of the predictions that the discriminator made from the fake images that this generator has created; and adding the MSE of the difference between the input-real image and the reconstructed one after passing through the two different generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################\n",
    "# Loss Functions #\n",
    "##################\n",
    "\n",
    "# MSE loss calculation between the input-real image and the reconstructed one after passing through two generators  \n",
    "const_loss_rp = tf.reduce_sum(tf.losses.mean_squared_error(batch_red_pandas, recon_rp))\n",
    "const_loss_l = tf.reduce_sum(tf.losses.mean_squared_error(batch_lions, recon_l))\n",
    "\n",
    "gen_rp_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_rp_fake, \n",
    "                                                                labels=tf.ones_like(disc_rp_fake)))\n",
    "gen_l_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_l_fake, \n",
    "                                                                labels=tf.ones_like(disc_l_fake)))\n",
    "\n",
    "disc_rp_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_rp_real, \n",
    "                                                                labels=tf.ones_like(disc_rp_real))) \\\n",
    "                            + tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_rp_fake, \n",
    "                                                                labels=tf.zeros_like(disc_rp_fake))\n",
    "disc_l_loss = tf.reduce_sum(tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_l_real, \n",
    "                                                                labels=tf.ones_like(disc_l_real))) \\\n",
    "                            + tf.nn.sigmoid_cross_entropy_with_logits(logits=disc_l_fake, \n",
    "                                                                labels=tf.zeros_like(disc_l_fake))\n",
    "\n",
    "# a value lambda is multiplied with const_loss_rp/l to give them more relevance, to get better reconstructions faster\n",
    "gen_loss = tf.constant(5.0)*tf.constant(250.0)*(const_loss_rp + const_loss_l) + gen_rp_loss + gen_l_loss\n",
    "disc_loss = disc_rp_loss + disc_l_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, after each epoch, we need to compute the gradients of both discriminators and generators. We'll compute gradients for the trainable variables taking into account the loss of both types of deep neural networks, and apply gradients to those variables.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# Compute & Apply gradients #\n",
    "#############################\n",
    "\n",
    "# Getting training variables (weights) of both generators and discriminators\n",
    "gen_rp_to_l_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"generator_rp_to_l\")\n",
    "gen_l_to_rp_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"generator_l_to_rp\")\n",
    "dis_rp_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"discriminator_rp\") \n",
    "dis_l_variables = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"discriminator_l\") \n",
    "\n",
    "# We will use the ADAM optimizer algorithm\n",
    "d_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "g_optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# The gradients will be computed in two turns, first those of the discriminator and next of the generators\n",
    "d_grads = d_optimizer.compute_gradients(disc_loss,dis_rp_variables + dis_l_variables) \n",
    "g_grads = g_optimizer.compute_gradients(gen_loss,gen_rp_to_l_variables + gen_l_to_rp_variables)\n",
    "\n",
    "# Finally, we will update the gradients\n",
    "update_D = d_optimizer.apply_gradients(d_grads)\n",
    "update_G = g_optimizer.apply_gradients(g_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's Train!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've got all we need! Input-data, a good looking architecture of neural networks, loss functions and a gradient computing algorithm. Now it's time to have patience, because around 90 epochs lasts an hour and the improvement is quite slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Training Time #\n",
    "#################\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "init2 = tf.local_variables_initializer()\n",
    "\n",
    "# we define a saver in order to save values of the trainable variables every 20 epochs\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # we initialize local and global variables\n",
    "    sess.run(init)\n",
    "    sess.run(init2)\n",
    "\n",
    "    # if a session is stored, we'll load it\n",
    "    try:\n",
    "        saver.restore(sess=sess, save_path=\"./DiscoGAN_logs/model.ckpt\")\n",
    "        print(\"\\n--------model restored--------\\n\")\n",
    "    except:\n",
    "        print(\"\\n--------model Not restored--------\\n\")\n",
    "        pass\n",
    "\n",
    "    # creates a coordinator to coordinate the termination of a set of threads\n",
    "    coord = tf.train.Coordinator()\n",
    "    # and threads start running \n",
    "    threads = tf.train.start_queue_runners(coord=coord)\n",
    "    \n",
    "    # we start itertating among epochs\n",
    "    for i in range(epoch):\n",
    "        \n",
    "        # in each epoch, trainable variables of generators are updated three times more than the discriminator's variables\n",
    "        # it's a way to impulse the generator to achieve better images and stay in tune with the discriminator's quality\n",
    "        for j in range(2):\n",
    "                _ = sess.run([update_G])\n",
    "        \n",
    "        # finally, we update the whole network and get 6 images, two of them real input images and the other 4 fakes\n",
    "        _, _, g_loss, d_loss, reconed_l, reconed_rp, fake_rp, fake_l,rp_image, l_image  = sess.run([update_G,update_D, \n",
    "                                                                                                gen_loss, disc_loss,\n",
    "                                                                                                recon_l,recon_rp,\n",
    "                                                                                                gen_rp_fake,gen_l_fake,\n",
    "                                                                                                batch_red_pandas,batch_lions])\n",
    "\n",
    "        # every 10 epochs the loss of the generators and discriminators are shown, to see how well they are doing\n",
    "        if i % 10 == 0:\n",
    "            print(\"{}th iter gen loss:{} disc loss:{}\".format(i,g_loss/batch_size, d_loss/batch_size))\n",
    "            \n",
    "        # every 20 epochs the model is saved next to the 6 images: 2 inputs, 2 generated chimeras and 2 reconstructions\n",
    "        if i % 20 == 0:\n",
    "            saver.save(sess, './DiscoGAN_logs/model.ckpt')\n",
    "            plt.imsave(\"./DiscoGAN_results/{}th_recon_l.png\".format(i),reconed_l[0])\n",
    "            plt.imsave(\"./DiscoGAN_results/{}th_recon_rp.png\".format(i),reconed_rp[0])\n",
    "            plt.imsave(\"./DiscoGAN_results/{}th_origin_rp.png\".format(i),rp_image[0])\n",
    "            plt.imsave(\"./DiscoGAN_results/{}th_origin_l.png\".format(i),l_image[0])\n",
    "            plt.imsave(\"./DiscoGAN_results/{}th_gen_rp.png\".format(i),fake_rp[0])\n",
    "            plt.imsave(\"./DiscoGAN_results/{}th_gen_l.png\".format(i),fake_l[0])\n",
    "    \n",
    "\n",
    "    # once it is finished, stop threads\n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. The idea of chimeras based on image segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This was our original idea, but it was too complex to make it real. As mentioned in our report, it consisted of 3 different parts, but we only implemented the first before discovering DiscoGAN (no pun intended)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The first part is about image segmentation. Our idea was to, given a picture, detect a red panda or lion in the image and detect where exactly was in the image. So we need a neural network capable to learn and return a segmentation of a given image between red panda, lion and background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fully Convolutional Networks (FCN) are mostly used in this area, and digging a little bit in github, we found a great FCN focused on detecting liquid, glass, mirrors and so on: https://github.com/sagieppel/Fully-convolutional-neural-network-FCN-for-semantic-segmentation-Tensorflow-implementation \n",
    "\n",
    "So, we thought of using that code in order to train this FCN to detect lions and red pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training FCN for image segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before doing anything, download our mentioned repository from GitHub and extract its content in the same directory as this notebook. The FCN is already trained, so there is no need to execute the training part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports to execute training and evaluation of the FCN\n",
    "sys.path.insert(0, 'FCN_code')\n",
    "\n",
    "import Data_Reader\n",
    "import BuildNetVgg16\n",
    "import CheckVGG16Model\n",
    "import OverrlayLabelOnImage as Overlay\n",
    "import TensorflowUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Input/Output Folders #\n",
    "########################\n",
    "\n",
    "Train_Image_Dir = \"Animals_FCN/\"  # Images and labels for training\n",
    "Train_Label_Dir = \"Animals_FCN_segmented\"  # Annotation in png format for training images\n",
    "logs_dir = \"FCN_logs/\"  # Path to logs directory where trained model and information will be stored\n",
    "if not os.path.exists(logs_dir):\n",
    "    os.makedirs(logs_dir)\n",
    "model_path = \"vgg16/vgg16.npy\"  # Path to pretrained vgg16 model for encoder\n",
    "CheckVGG16Model.CheckVGG16(model_path)  # Check if pretrained vgg16 model is available; if not, try to download "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Hyper-parameters #\n",
    "####################\n",
    "\n",
    "learning_rate = 1e-5  # Learning rate for Adam Optimizer\n",
    "TrainLossTxtFile = logs_dir + \"TrainLoss.txt\"  # Where train losses will be writen\n",
    "Batch_Size = 1  # Number of files per training iteration\n",
    "Weight_Loss_Rate = 5e-4  # Weight for the weight decay loss function\n",
    "MAX_ITERATION = 100000  # Max number of training iteration\n",
    "NUM_CLASSES = 3  # 0: Background - 1: Red Pandas - 2: Lions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Gradient Definition #\n",
    "#######################\n",
    "\n",
    "def train(loss_val, var_list):\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "    grads = optimizer.compute_gradients(loss_val, var_list=var_list)\n",
    "    return optimizer.apply_gradients(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "# Training Function #\n",
    "#####################\n",
    "\n",
    "def trainingFCN(argv=None):\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_probabilty\")  # Dropout probability\n",
    "\n",
    "    # Placeholders for input image and labels\n",
    "    image = tf.placeholder(tf.float32, shape=[None, None, None, 3], name=\"input_image\")  # Input image batch first dimension image number second dimension width third dimension height 4 dimension RGB\n",
    "    GTLabel = tf.placeholder(tf.int32, shape=[None, None, None, 1], name=\"GTLabel\")  # Ground truth labels for training\n",
    "\n",
    "    # Build FCN Net\n",
    "    Net = BuildNetVgg16.BUILD_NET_VGG16(vgg16_npy_path=model_path)  # Create class for the network\n",
    "    Net.build(image, NUM_CLASSES, keep_prob)  # Create the net and load intial weights\n",
    "\n",
    "    # Get loss functions for neural net work  one loss function for each set of label\n",
    "    Loss = tf.reduce_mean((tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.squeeze(GTLabel, squeeze_dims=[3]), logits=Net.Prob, name=\"Loss\")))  # Define loss function for training\n",
    "\n",
    "    trainable_var = tf.trainable_variables()  # Collect all trainable variables for the net\n",
    "    train_op = train(Loss, trainable_var)  # Create Train Operation for the net\n",
    "\n",
    "    # Create reader for data set\n",
    "    TrainReader = Data_Reader.Data_Reader(Train_Image_Dir,  GTLabelDir=Train_Label_Dir, BatchSize=Batch_Size)\n",
    "    \n",
    "    \n",
    "    sess = tf.Session()  # Start Tensorflow session\n",
    "\n",
    "    # load trained model if exist\n",
    "    print(\"Setting up Saver...\")\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer()) #Initialize variables\n",
    "    ckpt = tf.train.get_checkpoint_state(logs_dir)\n",
    "    if ckpt and ckpt.model_checkpoint_path: # if train model exist restore it\n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\"Model restored...\")\n",
    "\n",
    "    # Create files in order to save loss\n",
    "    f = open(TrainLossTxtFile, \"w\")\n",
    "    f.write(\"Iteration\\tloss\\t Learning Rate=\"+str(learning_rate))\n",
    "    f.close()\n",
    "\n",
    "    # Start Training loop: Main Training\n",
    "    for itr in range(MAX_ITERATION):\n",
    "        Images,  GTLabels = TrainReader.ReadAndAugmentNextBatch()  # Load augmeted images and ground true labels for training\n",
    "        feed_dict = {image: Images, GTLabel: GTLabels, keep_prob: 0.5}\n",
    "        sess.run(train_op, feed_dict=feed_dict)  # Train one cycle\n",
    "\n",
    "        # Save trained model\n",
    "        if itr % 500 == 0 and itr>0:\n",
    "            print(\"Saving Model to file in \"+logs_dir)\n",
    "            saver.save(sess, logs_dir + \"model.ckpt\", itr)  # Save model\n",
    "\n",
    "        # Write and display train loss\n",
    "        if itr % 10 == 0:\n",
    "            # Calculate train loss\n",
    "            feed_dict = {image: Images, GTLabel: GTLabels, keep_prob: 1}\n",
    "            TLoss = sess.run(Loss, feed_dict=feed_dict)\n",
    "            print(\"Step \"+str(itr)+\" Train Loss=\"+str(TLoss))\n",
    "            # Write train loss to file\n",
    "            with open(TrainLossTxtFile, \"a\") as f:\n",
    "                f.write(\"\\n\"+str(itr)+\"\\t\"+str(TLoss))\n",
    "                f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Run Script #\n",
    "##############\n",
    "\n",
    "trainingFCN()\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluating FCN for image segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you are going to test this FCN with some test images. Those images are already in Test directory and the output segmentations will be saved automatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Input/Output Folders #\n",
    "########################\n",
    "\n",
    "logs_dir= \"FCN_logs\" # path to logs directory where trained model and information will be stored\n",
    "Image_Dir=\"Lion_64x64/\" # CHANGE IT if you want, it's the test image folder\n",
    "Pred_Dir=\"FCN_results/\" # Directory where the output prediction will be written\n",
    "NameEnd=\"\" # Add this string to the ending of the file name optional\n",
    "model_path=\"vgg16/vgg16.npy\" # Path to pretrained vgg16 model for encoder\n",
    "CheckVGG16Model.CheckVGG16(model_path)# Check if pretrained vgg16 model avialable and if not try to download it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Hyperparameters #\n",
    "###################\n",
    "\n",
    "w=0.6 # weight of overlay on image\n",
    "NUM_CLASSES = 3 # Number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluatingFCN(argv=None):\n",
    "    \n",
    "    # Placeholders for input image and labels\n",
    "    keep_prob = tf.placeholder(tf.float32, name=\"keep_probabilty\")  # Dropout probability\n",
    "    image = tf.placeholder(tf.float32, shape=[None, None, None, 3], name=\"input_image\")  # Input image batch first dimension image number second dimension width third dimension height 4 dimension RGB\n",
    "\n",
    "    # Build net and load intial weights (weights before training)\n",
    "    Net = BuildNetVgg16.BUILD_NET_VGG16(vgg16_npy_path=model_path)  # Create class instance for the net\n",
    "    Net.build(image, NUM_CLASSES, keep_prob)  \n",
    "    \n",
    "    # Data reader for validation/testing images\n",
    "    ValidReader = Data_Reader.Data_Reader(Image_Dir,  BatchSize=1)\n",
    "    \n",
    "    sess = tf.Session() #Start Tensorflow session\n",
    "\n",
    "    print(\"Setting up Saver...\")\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    ckpt = tf.train.get_checkpoint_state(logs_dir)\n",
    "    # if train model exist restore it\n",
    "    if ckpt and ckpt.model_checkpoint_path: \n",
    "        saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        print(\"Model restored...\")\n",
    "    else:\n",
    "        print(\"ERROR NO TRAINED MODEL IN: \"+ckpt.model_checkpoint_path+\" See Train.py for creating train network \")\n",
    "        sys.exit()\n",
    "\n",
    "    # Create output directories for predicted label\n",
    "    if not os.path.exists(Pred_Dir): os.makedirs(Pred_Dir)\n",
    "    if not os.path.exists(Pred_Dir+\"/OverLay\"): os.makedirs(Pred_Dir+\"/OverLay\")\n",
    "    if not os.path.exists(Pred_Dir + \"/Label\"): os.makedirs(Pred_Dir + \"/Label\")\n",
    "\n",
    "    \n",
    "    print(\"Running Predictions:\")\n",
    "    print(\"Saving output to:\" + Pred_Dir)\n",
    "    # Go over all images and predict semantic segmentation in various classes\n",
    "    fim = 0\n",
    "    print(\"Start Predicting \" + str(ValidReader.NumFiles) + \" images\")\n",
    "    while (ValidReader.itr < ValidReader.NumFiles):\n",
    "        fim += 1      \n",
    "        print(str(fim * 100.0 / ValidReader.NumFiles) + \"%\")\n",
    "\n",
    "        # Load image\n",
    "        FileName=ValidReader.OrderedFiles[ValidReader.itr] # Get input image name\n",
    "        Images = ValidReader.ReadNextBatchClean() # Load testing image\n",
    "\n",
    "        # Predict annotation using net\n",
    "        LabelPred = sess.run(Net.Pred, feed_dict={image: Images, keep_prob: 1.0})\n",
    "        \n",
    "        # Save predicted labels overlay on images\n",
    "        misc.imsave(Pred_Dir + \"/OverLay/\"+ FileName+NameEnd  , Overlay.OverLayLabelOnImage(Images[0],LabelPred[0], w)) #Overlay label on image\n",
    "        misc.imsave(Pred_Dir + \"/Label/\" + FileName[:-4] + \".png\" + NameEnd, LabelPred[0].astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "# Run Script #\n",
    "##############\n",
    "\n",
    "evaluatingFCN()\n",
    "print(\"Finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
